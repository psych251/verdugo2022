---
title: Replication of Study 'Choice Boosts Curiosity' by Verdugo et al. (2022, Psychological
  Science)
author: "Austin Weideman"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '3'
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

##Introduction

The study 'Choice Boosts Curiosity' by Verdugo et al. (2022, Psychological Science) tested the effect of choice on curiosity when controlling for prechoice preference. Experiments tested if a participant's choice of a given lottery affected their subsequent curiosity about the results of the lottery. The target finding for replication is the increase in curiosity due to choice.

Link to githhub repository: <https://github.com/psych251/verdugo2022>

Link to original paper: <https://github.com/psych251/verdugo2022/blob/main/original_paper/verdugo2022.pdf>

Link to paradigm: <https://stanforduniversity.qualtrics.com/jfe/form/SV_eIETWpUoMKe1vHU>

##Methods

###Power Analysis

The authors concluded that a sample size of at least 34 "would allow [them] to detect a within-subjects difference of at least a medium effect size (Cohen's d \> 0.5) with 80% power, using a two-tailed t test" (Verdugo et al., 2022).

###Planned Sample

Sample size: n \>= 80 (much shorter length of experiment calls for more participants) Demographics: Age 20-40, English speaking

###Materials

Survey recreated in Qualtrics. Random lotteries with comparable expected values (all within 1 point) and comparable outcome uncertainties (all within 50 points) generated using a Python program I wrote. Participants recruited via Prolific. Data processed using R programming language.

###Procedure

The replication procedure follows the Experiment 1 procedure outlined in the original paper:

"To assess the effect of choice on curiosity, we designed an experimental task in which we manipulated, on a trial-to-trial basis, whether participants made a choice or not and measured their subsequent curiosity. In each trial of the task, participants saw two lotteries, but only one was selected and played. Lotteries were depicted in the form of vases containing marbles, accompanied by a label indicating the number of points associated with each marble. Each vase contained 20 marbles of two types: blue and red. A marble could be worth any number from 10 to 90 points, in steps of 10 (10 points, 20 points, 30 points, etc.). The two types of marbles within a lottery always had different values. The distribution of marbles of each type within the vase ranged from 5% to 95% in steps of five (5%--95% of vase, 10%--90% of vase, 15%--85% of vase, etc.). On some trials, the participant chose the selected lottery, whereas on other trials, the computer selected it for them. Participants were told that they would always earn a monetary payoff in proportion to the points associated with the outcome of the selected lottery but that they could not always see the outcome after each trial. At the end of each trial, we assessed participants' curiosity using ratings (Experiment 1: explicit curiosity)..."

"At the beginning of each trial in Experiment 1 (explicit curiosity), participants saw two lotteries in the form of vases and indicated which one they preferred (a). After that, one of the two lotteries was selected, either by the participant (choice; 50% of trials) or by the computer (no choice; 50% of trials). This selected lottery both determined the reward that participants would receive and was subsequently addressed in the latter curiosity assessment, potentially providing curiosity relief. In no-choice trials, the computer selected the lottery initially preferred by the participant half of the time (no choice/preferred; 25% of total trials) and the other lottery half of the time (no choice/not preferred; 25% of total trials). This distinction within no-choice trials was not made explicit to participants. After providing a response, participants saw the selected lottery. They then indicated how curious they were using a scale ranging from 1 (not curious) to 4 (very curious). Finally, participants saw a screen in which the outcome of the lottery was either shown (50% of trials) or hidden (50% of trials). Whether the outcome was shown or hidden was determined randomly and was not contingent on participants' curiosity responses. Participants were awarded the points corresponding to the outcome of the lottery in every trial (added to their total point sum), regardless of whether they got to see the outcome or not" (Verdugo et al., 2022).

See Differences from Original Study for exceptions made to procedure.

###Analysis Plan

Key measure: Average difference in self-reported curiosity between Choice/Preferred and No-Choice/Preferred trials

###Differences from Original Study

-   Only Experiment 1 from the original study will be replicated. Only the key claim of the paper (the difference in curiosity between Choice/Preferred and No-Choice/Preferred conditions) will be measured. Curiosity as a function of expected value and outcome uncertainty will not be discussed.
-   The experiment will be run as a survey in Qualtrics rather than via the Presentation software (as in original study).
-   Participants will not give a verbal summary of instructions. Instead, they will acknowledge having read written instructions and will participate in an interactive tutorial with two practice trials (one choice and one no-choice) rather than 20 practice trials as in the original study.
-   A smaller selection of lotteries will be included due to time constraint. There will be 6 unique lotteries, so 12 total trials (one choice and one no-choice for each lottery).
-   There will be no timed element to the survey in order to give participants time to answer questions accurately, taking into account that the initial training is far less intensive than in the original study, and the setting is remote rather than in-person. This will prevent haphazard responses that may occur with timed questions and missed responses due to auto-advancement.

### Methods Addendum (Post Data Collection)

#### Actual Sample

#### Differences from pre-data collection methods plan

##Results

### Data preparation

I will closely follow the data preparation outlined in the original paper:

"We filtered out trials in which participants missed or did not respond within the allocated times for any of the three required responses (0.37% of total trials in Experiment 1; 0.37% in Experiment 2). Furthermore, we filtered out choice trials in which participants had chosen a lottery other than their preferred one (2.73% of total trials in Experiment 1; 2.93% in Experiment 2). Because we expected participants to exhibit their preferences through their choices, we assumed that inconsistencies were due to changes of mind or response errors.
We sorted the remaining trials into three selection conditions: choice-preferred trials (48.61% of included trials in Experiment 1; 48.42% in Experiment 2), no-choice/preferred trials (25.72% of included trials in Experiment 1; 25.79% in Experiment 2), and no-choice/not-preferred trials (25.67% of included trials in Experiment 1; 25.79% in Experiment 2)" (Verdugo et al., 2022).

I will discount responses from participants who took far too long to complete the survey (>16 minutes) and those who were unreasonably fast (<4 and a half minutes). Within participant responses, I will not include trials in which participants chose a different vase than the one they preferred.

```{r}
# load data
library(tidyverse)
library(dplyr)
cc_data <- read.csv('/Users/austinweideman/Desktop/surveydata.csv')
data <- cc_data
view(data)
```

```{r}
all_participant_trials <- data.frame(matrix(ncol = 5, nrow = 0))

i = 1
num_valid_participants = 0
for (row in 3:nrow(data)) {
  participant_row <- data[c(row),]
  
  # move to next participant if duration of survey is unreasonably fast or slow
  # move to next participant if current participant did not complete survey
  duration <- strtoi(participant_row['Duration..in.seconds.'])
  finished <- participant_row['Finished']
  if (duration < 255 || duration > 1000 || finished == 'False') {
    next
  }
  else {
    num_valid_participants = num_valid_participants + 1
  }
  t1 = participant_row %>% dplyr::select(T1.C.Preference, T1.Choice.Vase, T1.C.Curiosity_1, T1.NC.Curiosity_1, T1Pref)
  t2 = participant_row %>% dplyr::select(T2.C.Preference, T2.Choice.Vase, T2.C.Curiosity_1, T2.NC.Curiosity_1, T2Pref)
  t3 = participant_row %>% dplyr::select(T3.C.Preference, T3.Choice.Vase, T3.C.Curiosity_1, T3.NC.Curiosity_1, T3Pref)
  t4 = participant_row %>% dplyr::select(T4.C.Preference, T4.Choice.Vase, T4.C.Curiosity_1, T4.NC.Curiosity_1, T4Pref)
  t5 = participant_row %>% dplyr::select(T5.C.Preference, T5.Choice.Vase, T5.C.Curiosity_1, T5.NC.Curiosity_1, T5Pref)
  t6 = participant_row %>% dplyr::select(T6.C.Preference, T6.Choice.Vase, T6.C.Curiosity_1, T6.NC.Curiosity_1, T6Pref)
  part_trials_data <- t1
  part_trials_data[nrow(part_trials_data) + 1,] = t2
  part_trials_data[nrow(part_trials_data) + 1,] = t3
  part_trials_data[nrow(part_trials_data) + 1,] = t4
  part_trials_data[nrow(part_trials_data) + 1,] = t5
  part_trials_data[nrow(part_trials_data) + 1,] = t6
  temp1 <- cbind(Trial = c('T1', 'T2', 'T3', 'T4', 'T5', 'T6'), part_trials_data)
  part_trials_data <- cbind(Participant = row-2, temp1)
  all_participant_trials = rbind(all_participant_trials, part_trials_data)
}
colnames(all_participant_trials) <- c('Participant', 'Trial', 'C_Preference', 'Choice', 'C_Curiosity', 'NC_Curiosity', 'NC_Preferred')
rownames(all_participant_trials) <- NULL

# below are all essential data from all 6 trials for each valid participant
all_participant_trials
# total number of valid participant
num_valid_participants
```

```{r}
# clean data by excluding trials in which preference does not match choice
all_participant_trials <- all_participant_trials %>% filter(C_Preference == Choice)
all_participant_trials
# select all no-choice/preferred trials for analysis
all_nc_preferred_trials <- all_participant_trials %>% filter(NC_Preferred == 'True')
all_nc_preferred_trials
# select all no-choice/not preferred trials for analysis
all_nc_not_pref_trials <- all_participant_trials %>% filter(NC_Preferred == 'False')
all_nc_not_pref_trials
```

```{r}
participant2 <- all_participant_trials %>% filter(Participant == '2')
participant2
```

```{r}
# here we will analyze responses from one participant (p2)
# we will calculate average curiosities in all trial conditions
c_sum_not_pref = 0
nc_sum_not_pref = 0
c_sum_pref = 0
nc_sum_pref = 0
divisor = 0
divisor2 = 0
for (row in 1:nrow(participant2)) {
  pref <- participant2[row, 'NC_Preferred']
  c_curiosity <- participant2[row, 'C_Curiosity']
  nc_curiosity <- participant2[row, 'NC_Curiosity']
  c_curiosity <- strtoi(c_curiosity)
  nc_curiosity <- strtoi(nc_curiosity)
  if (pref == 'True') {
    c_sum_pref <- c_sum_pref + c_curiosity
    nc_sum_pref <- nc_sum_pref + nc_curiosity
    divisor = divisor + 1
  }
  if (pref == 'False') {
    c_sum_not_pref <- c_sum_not_pref + c_curiosity
    nc_sum_not_pref <- nc_sum_not_pref + nc_curiosity
    divisor2 = divisor2 + 1
  }
}
# average choice/preferred curiosity rating for those games with a matching no-choice/preferred game from this participant
avg_c_pref <- c_sum_pref / divisor
avg_c_pref

# average no-choice/preferred curiosity rating for the same three games for the same participant
avg_nc_pref <- nc_sum_pref / divisor
avg_nc_pref

# average no-choice/not-preferred curiosity rating for all nc/not-preferred trials from this participant
avg_nc_not_pref <- nc_sum_not_pref / divisor2
avg_nc_not_pref

table <- c(avg_c_pref, avg_nc_pref, avg_nc_not_pref)
barplot(table, main = 'Average Curiosity Ratings for Participant 2', names.arg=c('Choice/Pref', 'No-Choice/Pref', 'No-Choice/Not-Pref'))
```

```{r}
c_sum_pref = 0
nc_sum_pref = 0

for (row in 1:nrow(all_nc_preferred_trials)) {
  pref <- all_nc_preferred_trials[row, 'NC_Preferred']
  c_curiosity <- strtoi(all_nc_preferred_trials[row, 'C_Curiosity'])
  nc_curiosity <- strtoi(all_nc_preferred_trials[row, 'NC_Curiosity'])
  c_sum_pref <- c_sum_pref + c_curiosity
  nc_sum_pref <- nc_sum_pref + nc_curiosity
}

c_all_sum = 0
for (row in 1:nrow(all_participant_trials)) {
  c_curiosity <- strtoi(all_participant_trials[row, 'C_Curiosity'])
  c_all_sum <- c_all_sum + c_curiosity
}

nc_notpref_sum = 0
for (row in 1:nrow(all_nc_not_pref_trials)) {
  nc_curiosity <- strtoi(all_participant_trials[row, 'NC_Curiosity'])
  nc_notpref_sum <- nc_notpref_sum + nc_curiosity
}

# average choice/preferred curiosity ratings across ALL trials (including those with NC/not preferred trials), not just those with a matching NC/Preferred game
avg_c_curiosity <- c_all_sum / (nrow(all_participant_trials))
avg_c_curiosity

# average choice/preferred curiosity rating for those trials with a matching no-choice/preferred game from all participants
avg_c_pref <- c_sum_pref / (nrow(all_nc_preferred_trials))
avg_c_pref

# average no-choice/preferred curiosity rating for the same games
avg_nc_pref <- nc_sum_pref / (nrow(all_nc_preferred_trials))
avg_nc_pref

# average no-choice/not-preferred curiosity rating for all nc/not-preferred trials
avg_nc_not_pref <- nc_notpref_sum / (nrow(all_nc_not_pref_trials))
avg_nc_not_pref

# barplot
table <- c(avg_c_pref, avg_nc_pref, avg_nc_not_pref)
barplot(table, main = 'Average Curiosity Ratings (All Participants)', names.arg=c('Choice/Pref', 'No-Choice/Pref', 'No-Choice/Not-Pref'))
```

```{r include=F}
###Data Preparation

####Load Relevant Libraries and Functions

####Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Confirmatory analysis

*Side-by-side graph with original graph is ideal here*

###Exploratory analyses

## Discussion

### Summary of Replication Attempt

### Commentary

