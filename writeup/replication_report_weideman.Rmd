---
title: Replication of Study 'Choice Boosts Curiosity' by Verdugo et al. (2022, Psychological
  Science)
author: "Austin Weideman"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '3'
---


## Introduction

The study 'Choice Boosts Curiosity' by Verdugo et al. (2022, Psychological Science) tested the effect of choice on curiosity independent of pre-choice preference. Experiments tested if a participant's choice of a given lottery affected their subsequent curiosity about the results of the lottery. The target finding for replication is the increase in curiosity due to choice.

Link to githhub repository: <https://github.com/psych251/verdugo2022>

Link to original paper:
Github: <https://github.com/psych251/verdugo2022/blob/main/original_paper/verdugo2022.pdf>
Sagepub: https://journals.sagepub.com/doi/full/10.1177/09567976221082637

Link to paradigm: <https://stanforduniversity.qualtrics.com/jfe/form/SV_eIETWpUoMKe1vHU>

## Methods

### Power Analysis

The authors concluded that a sample size of at least 34 "would allow [them] to detect a within-subjects difference of at least a medium effect size (Cohen's d \> 0.5) with 80% power, using a two-tailed t test" (Verdugo et al., 2022). However, keeping in mind the number of trials done in the original study (116 per participant), the total number of data points is actually relatively high. Considering I only have 6 trials (12 games, one choice, one no choice for each trial), I need to recruit more participants to compensate for this lack of data points.

### Planned Sample

Sample size: N = 83 (much fewer number of trials calls for more participants).
Budget limitations prevent a larger sample size.
Demographics: Age 20-40, English speaking

### Materials

Survey recreated in Qualtrics. Random lotteries with comparable expected values (all within 1 point) and comparable outcome uncertainties (all within 50 points) generated using a Python program I wrote. Participants recruited via Prolific. Data processed using R programming language.

### Procedure

The replication procedure follows the Experiment 1 procedure outlined in the original paper:

"To assess the effect of choice on curiosity, we designed an experimental task in which we manipulated, on a trial-to-trial basis, whether participants made a choice or not and measured their subsequent curiosity. In each trial of the task, participants saw two lotteries, but only one was selected and played. Lotteries were depicted in the form of vases containing marbles, accompanied by a label indicating the number of points associated with each marble. Each vase contained 20 marbles of two types: blue and red. A marble could be worth any number from 10 to 90 points, in steps of 10 (10 points, 20 points, 30 points, etc.). The two types of marbles within a lottery always had different values. The distribution of marbles of each type within the vase ranged from 5% to 95% in steps of five (5%--95% of vase, 10%--90% of vase, 15%--85% of vase, etc.). On some trials, the participant chose the selected lottery, whereas on other trials, the computer selected it for them. Participants were told that they would always earn a monetary payoff in proportion to the points associated with the outcome of the selected lottery but that they could not always see the outcome after each trial. At the end of each trial, we assessed participants' curiosity using ratings (Experiment 1: explicit curiosity)..."

"At the beginning of each trial in Experiment 1 (explicit curiosity), participants saw two lotteries in the form of vases and indicated which one they preferred (a). After that, one of the two lotteries was selected, either by the participant (choice; 50% of trials) or by the computer (no choice; 50% of trials). This selected lottery both determined the reward that participants would receive and was subsequently addressed in the latter curiosity assessment, potentially providing curiosity relief. In no-choice trials, the computer selected the lottery initially preferred by the participant half of the time (no choice/preferred; 25% of total trials) and the other lottery half of the time (no choice/not preferred; 25% of total trials). This distinction within no-choice trials was not made explicit to participants. After providing a response, participants saw the selected lottery. They then indicated how curious they were using a scale ranging from 1 (not curious) to 4 (very curious). Finally, participants saw a screen in which the outcome of the lottery was either shown (50% of trials) or hidden (50% of trials). Whether the outcome was shown or hidden was determined randomly and was not contingent on participants' curiosity responses. Participants were awarded the points corresponding to the outcome of the lottery in every trial (added to their total point sum), regardless of whether they got to see the outcome or not" (Verdugo et al., 2022).

See Differences from Original Study for exceptions made to procedure.

### Analysis Plan

Key analysis of interest: Contrast in self-reported curiosity between Choice/Preferred and No-Choice/Preferred conditions (coefficient reported from brms function with 95% CI).

### Differences from Original Study

-   Only Experiment 1 from the original study will be replicated. Only the key claim of the paper (the difference in curiosity between Choice/Preferred and No-Choice/Preferred conditions) will be measured. Curiosity as a function of expected value and outcome uncertainty will not be discussed.
-   The experiment will be run as a survey in Qualtrics rather than via the Presentation software (as in original study).
- The sample will likely be different. I believe the original sample consisted only of people from the Netherlands. The sample of the original experiment was as follows: "The final sample in Experiment 1 (N = 34) included 26 women (age: M = 23.5 years, SD = 4.5); the final sample in Experiment 2 (N = 34) included 24 women (age: M = 23.5 years, SD = 3.1). All participants had normal or corrected-to-normal vision. Both experiments were approved by the local ethics committee (CMO Arnhem-Nijmegen, The Netherlands) under the general ethics approval for standard studies conducted at the Donders Centre for Cognitive Neuroimaging (CMO 2019/288 v. 2.2). Prior to participation, all participants gave written informed consent according to the Declaration of Helsinki." (Verdugo et al., 2022).
-   Participants will not give a verbal summary of instructions. Instead, they will acknowledge having read written instructions and will participate in an interactive tutorial with two practice trials (one choice and one no-choice) rather than 20 practice trials as in the original study.
- Bonus payment will be directly proportional to number of points, ie. for every 10 points, a participant earns 1 cent bonus. Average bonus is about 50 cents per participant.
-   A smaller selection of lotteries will be included due to time constraint. There will be 6 unique lotteries, so 12 total trials (one choice and one no-choice for each lottery).
-   There will be no explicit time pressure to the survey in order to give participants time to answer questions accurately, taking into account that the initial training is far less intensive than in the original study, and the setting is remote rather than in-person. This will prevent haphazard responses that may occur with timed questions and missed responses due to auto-advancement. With that being said, the data exclusion criteria do exclude participants who completed the survey unreasonably fast or slow.

### Methods Addendum (Post Data Collection)

#### Actual Sample

#### Differences from pre-data collection methods plan

## Results

### Data preparation

I will closely follow the data preparation and exclusion criteria outlined in the original paper:

"Thirty-seven healthy individuals participated in Experiment 1, and another 40 healthy individuals participated in Experiment 2. Following our preregistration (https://osf.io/gseqn/), we excluded participants who missed more than 10% of total trials (Criterion 1). Additionally, we excluded participants who scored poorly according to at least one of two other performance criteria. Criterion 2 was when a participant gave the same curiosity rating in more than two thirds of their responses in Experiment 1 or made the same willingness-to-wait decision in more than 90% of their responses in Experiment 2. Criterion 3 was when a participant ranked lower than 3 standard deviations below the mean in choice-preference coherence (i.e., frequency of trials on which a participant chose the same vase that they had previously indicated that they preferred). We excluded three participants from Experiment 1 (one on the basis of Criterion 2 and two on the basis of Criterion 3) and six participants from Experiment 2 (five on the basis of Criterion 2 and one on the basis of Criterion 3). No participants were excluded on the basis of Criterion 1. The final sample in Experiment 1 (N = 34) included 26 women (age: M = 23.5 years, SD = 4.5); the final sample in Experiment 2 (N = 34) included 24 women (age: M = 23.5 years, SD = 3.1)" (Verdugo et al., 2022).

I will follow the criterion of exluding participants who gave the same curiosity rating in more than two thirds of their responses. However, instead of excluding participants who missed more than 10% of 116 trials (as in the original experiment), I will simply exclude those who did not finish all of 6 trials (12 total games) in this experiment, which, since it is far fewer than in the original and not subject to timed auto-advancement, can have a stricter exclusion criterion.

"We filtered out trials in which participants missed or did not respond within the allocated times for any of the three required responses (0.37% of total trials in Experiment 1; 0.37% in Experiment 2). Furthermore, we filtered out choice trials in which participants had chosen a lottery other than their preferred one (2.73% of total trials in Experiment 1; 2.93% in Experiment 2). Because we expected participants to exhibit their preferences through their choices, we assumed that inconsistencies were due to changes of mind or response errors.
We sorted the remaining trials into three selection conditions: choice-preferred trials (48.61% of included trials in Experiment 1; 48.42% in Experiment 2), no-choice/preferred trials (25.72% of included trials in Experiment 1; 25.79% in Experiment 2), and no-choice/not-preferred trials (25.67% of included trials in Experiment 1; 25.79% in Experiment 2)" (Verdugo et al., 2022).

I will discount responses from participants who took far too long to complete the survey (>18 minutes) and those who were unreasonably fast (<4 and a half minutes). Within participant responses, I will not include trials in which participants chose a different vase than the one they preferred. Finally, I will sort the remaining data into the same three selection conditions.

```{r}
# load data
library(tidyverse)
library(dplyr)
cc_data <- read.csv('/Users/austinweideman/Desktop/pilotbdata.csv')
data <- cc_data
# view(data)
```

```{r}
# organize all participant trial data into table
all_participant_trials <- data.frame(matrix(ncol = 5, nrow = 0))

# create the table, keeping in mind exclusions
excluded_participants <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(excluded_participants) <- cbind('Excluded Participant', 'Reason')
i = 1
num_participants = 0
for (row in 3:nrow(data)) {
  participant_row <- data[c(row),]
  participant <- row-2
  num_participants = num_participants + 1

  duration <- strtoi(participant_row['Duration..in.seconds.'])
  finished <- participant_row['Finished']
  
  # move to next participant if duration of survey is unreasonably fast or slow
  # or if current participant did not complete survey
  if (duration < 270) {
    excluded_part <- c(participant, 'Too Fast')
    excluded_participants[nrow(excluded_participants)+1,] <- excluded_part
    next
  }
  if (duration > 1080) {
    excluded_part <- c(participant, 'Too Slow')
    excluded_participants[nrow(excluded_participants)+1,] <- excluded_part
    next
  }
  if (finished == 'False') {
    excluded_part <- c(participant, 'Unfinished')
    excluded_participants[nrow(excluded_participants)+1,] <- excluded_part
    next
  }
  t1 = participant_row %>% dplyr::select(T1.C.Preference, T1.Choice.Vase, T1.C.Curiosity_1, T1.NC.Curiosity_1, T1Pref)
  t2 = participant_row %>% dplyr::select(T2.C.Preference, T2.Choice.Vase, T2.C.Curiosity_1, T2.NC.Curiosity_1, T2Pref)
  t3 = participant_row %>% dplyr::select(T3.C.Preference, T3.Choice.Vase, T3.C.Curiosity_1, T3.NC.Curiosity_1, T3Pref)
  t4 = participant_row %>% dplyr::select(T4.C.Preference, T4.Choice.Vase, T4.C.Curiosity_1, T4.NC.Curiosity_1, T4Pref)
  t5 = participant_row %>% dplyr::select(T5.C.Preference, T5.Choice.Vase, T5.C.Curiosity_1, T5.NC.Curiosity_1, T5Pref)
  t6 = participant_row %>% dplyr::select(T6.C.Preference, T6.Choice.Vase, T6.C.Curiosity_1, T6.NC.Curiosity_1, T6Pref)
  part_trials_data <- t1
  part_trials_data[nrow(part_trials_data) + 1,] = t2
  part_trials_data[nrow(part_trials_data) + 1,] = t3
  part_trials_data[nrow(part_trials_data) + 1,] = t4
  part_trials_data[nrow(part_trials_data) + 1,] = t5
  part_trials_data[nrow(part_trials_data) + 1,] = t6
  temp1 <- cbind(Trial = c('T1', 'T2', 'T3', 'T4', 'T5', 'T6'), part_trials_data)
  part_trials_data <- cbind(Participant = participant, temp1)
  all_participant_trials = rbind(all_participant_trials, part_trials_data)
}
colnames(all_participant_trials) <- c('Participant', 'Trial', 'C_Preference', 'Choice', 'C_Curiosity', 'NC_Curiosity', 'NC_Preferred')
rownames(all_participant_trials) <- NULL

# number of participants
num_participants

# excluded participants
excluded_participants

# essential data from all 6 trials for all participants
all_participant_trials
```

```{r}
# clean data by excluding participants who gave the same curiosity rating for more than two thirds of their responses

filtered_participant_trials <- all_participant_trials
num_valid_participants <- num_participants
participants <- all_participant_trials$Participant
participants <- participants[!duplicated(participants)]

for (participant in participants) {
  num1 <- 0
  num2 <- 0
  num3 <- 0
  num4 <- 0
  part_data <- all_participant_trials %>% filter(Participant == participant)
  for (row in 1: nrow(part_data)) {
    if (part_data[row, 'C_Curiosity'] == '1') {
      num1 <- num1 + 1
    }
    if (part_data[row, 'NC_Curiosity'] == '1') {
      num1 <- num1 + 1
    }
    if (part_data[row, 'C_Curiosity'] == '2') {
      num2 <- num2 + 1
    }
    if (part_data[row, 'NC_Curiosity'] == '2') {
      num2 <- num2 + 1
    }
    if (part_data[row, 'C_Curiosity'] == '3') {
      num3 <- num3 + 1
    }
    if (part_data[row, 'NC_Curiosity'] == '3') {
      num3 <- num3 + 1
    }
    if (part_data[row, 'C_Curiosity'] == '4') {
      num4 <- num4 + 1
    }
    if (part_data[row, 'NC_Curiosity'] == '4') {
      num4 <- num4 + 1
    }
  }
  counts <- c(num1, num2, num3, num4)
  if (max(counts) > 8) {
    filtered_participant_trials <- filtered_participant_trials %>% filter(Participant != participant)
    excluded_part <- c(participant, 'Too many similar responses')
    excluded_participants[nrow(excluded_participants)+1,] <- excluded_part
    num_valid_participants <- num_valid_participants - 1
  }
}
# total number of valid participants
num_valid_participants

# excluded participants
excluded_participants

# filtered data
filtered_participant_trials
```

```{r}
# further clean data by excluding those trials in which preference does not match choice
filtered_participant_trials <- filtered_participant_trials %>% filter(C_Preference == Choice)

filtered_participant_trials
```

```{r}
# we can now make this data more compact by removing vase information since it is no longer necessary
filtered_participant_trials <- subset(filtered_participant_trials, select = -c(C_Preference, Choice))
filtered_participant_trials
```
```{r}
# we can convert this data to long form for later analysis
# we will have one column for conditions, and one for curiosity

filtered_participant_trials_long <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(filtered_participant_trials_long) <- c('Participant', 'Trial', 'Conditions', 'Curiosity')

for (row in 1:nrow(filtered_participant_trials)) {
  part_row <- filtered_participant_trials[row,]
  participant <- part_row[1, 'Participant']
  trial <- part_row[1, 'Trial']
  c_cur <- strtoi(part_row[1, 'C_Curiosity'])
  nc_cur <- strtoi(part_row[1, 'NC_Curiosity'])
  new_choice_row <- c(participant, trial, 'Choice/Preferred', c_cur)
  filtered_participant_trials_long[nrow(filtered_participant_trials_long)+1,] = new_choice_row
  nc_condition <- part_row[1, 'NC_Preferred']
  if (nc_condition == 'True') {
    new_nc_pref_row <- c(participant, trial, 'No_Choice/Preferred', nc_cur)
    filtered_participant_trials_long[nrow(filtered_participant_trials_long)+1,] = new_nc_pref_row
  }
  if (nc_condition == 'False') {
    new_nc_not_pref_row <- c(participant, trial, 'No_Choice/Not_Preferred', nc_cur)
    filtered_participant_trials_long[nrow(filtered_participant_trials_long)+1,] = new_nc_not_pref_row
  }
}
filtered_participant_trials_long$Curiosity <- as.numeric(filtered_participant_trials_long$Curiosity)
filtered_participant_trials_long


```
```{r}
# wide format for contrasts with brms
# set contrast codes for three variables: is.choice_pref, is.nochoice.pref, and is.nochoice_notpref
filtered_participant_trials_wide <- data.frame(matrix(ncol = 6, nrow = 0))
colnames(filtered_participant_trials_wide) <- c('Participant', 'Trial', 'is.choice_pref', 'is.nochoice_pref', 'is.nochoice_notpref', 'Curiosity')

for (row in 1:nrow(filtered_participant_trials)) {
  part_row <- filtered_participant_trials[row,]
  participant <- part_row[1, 'Participant']
  trial <- part_row[1, 'Trial']
  c_cur <- strtoi(part_row[1, 'C_Curiosity'])
  nc_cur <- strtoi(part_row[1, 'NC_Curiosity'])
  new_choice_row <- c(participant, trial, 0.5, 0, NA, c_cur)
  filtered_participant_trials_wide[nrow(filtered_participant_trials_wide)+1,] = new_choice_row
  nc_condition <- part_row[1, 'NC_Preferred']
  if (nc_condition == 'True') {
    new_nc_pref_row <- c(participant, trial, -0.5, 0.5, NA, nc_cur)
    filtered_participant_trials_wide[nrow(filtered_participant_trials_wide)+1,] = new_nc_pref_row
  }
  if (nc_condition == 'False') {
    new_nc_not_pref_row <- c(participant, trial, 0, -0.5, NA, nc_cur)
    filtered_participant_trials_wide[nrow(filtered_participant_trials_wide)+1,] = new_nc_not_pref_row
  }
}
# convert to numeric
filtered_participant_trials_wide$is.choice_pref <- as.numeric(filtered_participant_trials_wide$is.choice_pref)
filtered_participant_trials_wide$is.nochoice_pref <- as.numeric(filtered_participant_trials_wide$is.nochoice_pref)
filtered_participant_trials_wide$is.nochoice_notpref <- as.numeric(filtered_participant_trials_wide$is.nochoice_notpref)
filtered_participant_trials_wide$Curiosity <- as.numeric(filtered_participant_trials_wide$Curiosity)
filtered_participant_trials_wide
```

```{r}
# select all choice/preferred games
choice_preferred_games_long <- filtered_participant_trials_long %>% filter(Conditions == 'Choice/Preferred')
choice_preferred_games_long
# average choice/preferred curiosity rating and standard deviation
avg_all_c_curiosity <- mean(choice_preferred_games_long$Curiosity)
sd_all_c_curiosity <- sd(choice_preferred_games_long$Curiosity)
avg_all_c_curiosity
sd_all_c_curiosity

# select all no-choice/preferred games for analysis
nc_preferred_trials <- filtered_participant_trials %>% filter(NC_Preferred == 'True')
nc_preferred_games_long <- filtered_participant_trials_long %>% filter(Conditions == 'No_Choice/Preferred')
nc_preferred_games_long
# average no choice/preferred curiosity rating and standard deviation
avg_all_nc_pref_curiosity <- mean(nc_preferred_games_long$Curiosity)
sd_all_nc_pref_curiosity <- sd(nc_preferred_games_long$Curiosity)
avg_all_nc_pref_curiosity
sd_all_nc_pref_curiosity

# select all no-choice/not preferred games for analysis
nc_not_pref_trials <- filtered_participant_trials %>% filter(NC_Preferred == 'False')
nc_not_pref_games_long <- filtered_participant_trials_long %>% filter(Conditions == 'No_Choice/Not_Preferred')
nc_not_pref_games_long
# average no choice/not preferred curiosity rating and standard deviation
avg_all_nc_not_pref_curiosity <- mean(nc_not_pref_games_long$Curiosity)
sd_all_nc_not_pref_curiosity <- sd(nc_not_pref_games_long$Curiosity)
avg_all_nc_not_pref_curiosity
sd_all_nc_not_pref_curiosity
```

```{r}
# plot average curioisities across all participants
df <- data.frame(Curiosity= c(round(avg_all_c_curiosity, 2), round(avg_all_nc_pref_curiosity, 2), round(avg_all_nc_not_pref_curiosity, 2)), sd = c(sd_all_c_curiosity, sd_all_nc_pref_curiosity, sd_all_nc_not_pref_curiosity), Conditions = c('Choice/Preferred', 'No Choice/Preferred', 'No Choice/Not Preferred'))
library(ggplot2)
plot <- ggplot(data=df, aes(x= reorder(Conditions, -Curiosity), y=Curiosity)) +
  geom_bar(width = 0.7, stat="identity", fill=c('#D2042D', '#0047AB', '#808080'))+
  geom_text(aes(label=Curiosity), hjust = 2, vjust=1.6, color="white", size=3.5)+
  theme_minimal() + geom_errorbar(aes(ymin=Curiosity-sd, ymax=Curiosity+sd), width = 0.2)
plot <- plot + ggtitle("Average Curiosity Ratings Across All Participants") +
  xlab("") + ylab("Curiosity Rating") + theme(plot.title = element_text(hjust = 0.5)) + scale_y_continuous(breaks = seq(0, 4.0, 0.5)) + theme(plot.title = element_text(color = 'black', size = 14, face = "bold"))
plot
```


```{r}
# isolate score, reward, and prolific id for each participant
# commented out to avoid showing PROLIFIC_PIDs online
# participant_rewards <- data.frame(matrix(ncol = 3, nrow = 0))
# 
# for (row in 3:nrow(data)) {
#   participant_row <- data[c(row),]
#   score_info = participant_row %>% dplyr::select(PROLIFIC_PID, Score, Reward)
#   scores <- cbind(Participant = row-2, score_info)
#   participant_rewards = rbind(participant_rewards, scores)
# }
# rownames(participant_rewards) <- NULL
# participant_rewards

```

```{r}
# create a table showing average curiosity ratings in all 3 conditions for each participant

participant_averages <- data.frame(matrix(ncol = 4, nrow = 0))

participants <- filtered_participant_trials$Participant
participants <- participants[!duplicated(participants)]

for (cur_participant in participants) {
  # filter for just this participant
  participant_data <- filtered_participant_trials %>% filter(Participant == cur_participant)
  
  # isolate no choice/preferred and no choice/not preferred trials
  p_nc_pref <- participant_data %>% filter(NC_Preferred == 'True')
  p_nc_not_pref <- participant_data %>% filter(NC_Preferred == 'False')
  
  # take the mean of all curiosities
  avg_c_curiosity <- mean(as.numeric(participant_data$C_Curiosity))
  avg_nc_pref_curiosity <- mean(as.numeric(p_nc_pref$NC_Curiosity))
  avg_nc_not_pref_curiosity <- mean(as.numeric(p_nc_not_pref$NC_Curiosity))
  
  participant_averages[nrow(participant_averages) + 1,] = c(cur_participant, avg_c_curiosity, avg_nc_pref_curiosity, avg_nc_not_pref_curiosity)
}
rownames(participant_averages) <- NULL
colnames(participant_averages) <- c('Participant', 'Avg_C_Curiosity', 'Avg_NC_Pref_Curiosity', 'Avg_NC_Not_Pref_Curiosity')
participant_averages
```

```{r}
# if necessary, we can isolate one participant's data
participant2 <- participant_averages %>% filter(Participant == 2)
participant2
```

```{r}
# analysis: run brm with curiosity modeled on different selection conditions
library(brms)

brm.fit <- brm(Curiosity ~ is.choice_pref + is.nochoice_pref, data = filtered_participant_trials_wide, family = cumulative("logit"))

summary(brm.fit)
```


### Confirmatory analysis

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

## Discussion

### Summary of Replication Attempt

### Commentary

