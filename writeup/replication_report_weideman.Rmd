---
title: Replication of Study 'Choice Boosts Curiosity' by Verdugo et al. (2022, Psychological
  Science)
author: "Austin Weideman"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '3'
---


## Introduction

The study 'Choice Boosts Curiosity' by Verdugo et al. (2022, Psychological Science) tested the effect of choice on curiosity independent of prechoice preference. In other words, the authors wanted to know if people are more curious about a preferred option which they choose themselves than a preferred option which is chosen for them. Experiments tested if a participant's choice of a given lottery affected their subsequent curiosity about the results of the lottery. The authors found that people exhibited greater curiosity about things that they chose themselves rather than things that were chosen for them. The finding for replication is this statistically significant contrast in curiosity between conditions of choice and no choice, spefically that "choice enhanced curiosity; participants were more curious about lotteries that they had chosen themselves... This was the case when curiosity was measured explicitly (brms contrast: 95% CI = [0.46, 0.71]" (Verdugo et al, 2022).

Link to githhub repository: <https://github.com/psych251/verdugo2022>

Link to original paper:
Github: <https://github.com/psych251/verdugo2022/blob/main/original_paper/verdugo2022.pdf>
Sagepub: https://journals.sagepub.com/doi/full/10.1177/09567976221082637

Link to paradigm: <https://stanforduniversity.qualtrics.com/jfe/form/SV_eIETWpUoMKe1vHU>

Link to preregistration: https://osf.io/85vx7/

## Methods

### Power Analysis

The key statistic is the associated 95% posterior credible interval for the contrast in curiosity ratings (using brms with family cumulative logit) between choice/preferred and no choice/preferred conditions. The authors concluded that a sample size of at least 34 "would allow [them] to detect a within-subjects difference of at least a medium effect size (Cohen's d \> 0.5) with 80% power, using a two-tailed t test" (Verdugo et al., 2022). However, keeping in mind the number of trials done in the original study (116 per participant), the total number of data points is relatively high. Considering my implementation only has 6 trials (12 games, one choice, one no choice for each trial) to keep the length reasonable for remote users, I will recruit more participants to compensate for the lack of data points.

### Planned Sample

The original sample was outlined as follows: "The final sample in Experiment 1 (N = 34) included 26 women (age: M = 23.5 years, SD = 4.5); the final sample in Experiment 2 (N = 34) included 24 women (age: M = 23.5 years, SD = 3.1). All participants had normal or corrected-to-normal vision. Both experiments were approved by the local ethics committee (CMO Arnhem-Nijmegen, The Netherlands) under the general ethics approval for standard studies conducted at the Donders Centre for Cognitive Neuroimaging (CMO 2019/288 v. 2.2). Prior to participation, all participants gave written informed consent according to the Declaration of Helsinki." (Verdugo et al., 2022).

The planned sample for this replication is N = 83, as the fewer number of trials call for more participants, but budget limitations prevent a larger sample size. Participants will be age 20-40, English speaking, and of US nationality, as this is the sample I am interested in studying.

### Materials

The lottery game was recreated in Qualtrics. Random pairs of lotteries containing vases with comparable expected values (all within 1 point) and comparable outcome uncertainties (all within 50 points) were generated using a Python program I wrote. Pairs of lotteries, however, varied in expected value and outcome uncertainty to ensure a diversity of games. Participants were recruited via Prolific. Data was processed using R statistical programming language.

### Procedure

The replication procedure closely follows the Experiment 1 procedure outlined in the original paper as follows:

"To assess the effect of choice on curiosity, we designed an experimental task in which we manipulated, on a trial-to-trial basis, whether participants made a choice or not and measured their subsequent curiosity. In each trial of the task, participants saw two lotteries, but only one was selected and played. Lotteries were depicted in the form of vases containing marbles, accompanied by a label indicating the number of points associated with each marble. Each vase contained 20 marbles of two types: blue and red. A marble could be worth any number from 10 to 90 points, in steps of 10 (10 points, 20 points, 30 points, etc.). The two types of marbles within a lottery always had different values. The distribution of marbles of each type within the vase ranged from 5% to 95% in steps of five (5%--95% of vase, 10%--90% of vase, 15%--85% of vase, etc.). On some trials, the participant chose the selected lottery, whereas on other trials, the computer selected it for them. Participants were told that they would always earn a monetary payoff in proportion to the points associated with the outcome of the selected lottery but that they could not always see the outcome after each trial. At the end of each trial, we assessed participants' curiosity using ratings (Experiment 1: explicit curiosity)..."

"At the beginning of each trial in Experiment 1 (explicit curiosity), participants saw two lotteries in the form of vases and indicated which one they preferred (a). After that, one of the two lotteries was selected, either by the participant (choice; 50% of trials) or by the computer (no choice; 50% of trials). This selected lottery both determined the reward that participants would receive and was subsequently addressed in the latter curiosity assessment, potentially providing curiosity relief. In no-choice trials, the computer selected the lottery initially preferred by the participant half of the time (no choice/preferred; 25% of total trials) and the other lottery half of the time (no choice/not preferred; 25% of total trials). This distinction within no-choice trials was not made explicit to participants. After providing a response, participants saw the selected lottery. They then indicated how curious they were using a scale ranging from 1 (not curious) to 4 (very curious). Finally, participants saw a screen in which the outcome of the lottery was either shown (50% of trials) or hidden (50% of trials). Whether the outcome was shown or hidden was determined randomly and was not contingent on participants' curiosity responses. Participants were awarded the points corresponding to the outcome of the lottery in every trial (added to their total point sum), regardless of whether they got to see the outcome or not" (Verdugo et al., 2022).

See Differences from Original Study for exceptions made to procedure.

### Analysis Plan

The analysis plan for the replication closely follows part of the analysis done in the original paper, which is outlined as follows:

First, they "read in, concatenated, filtered, and summarized the data using RStudio". See "Data Preparation" for details regarding exclusions.

"We sorted... trials into three selection conditions: choice-preferred trials (48.61% of included trials in Experiment 1; 48.42% in Experiment 2), no-choice/preferred trials (25.72% of included trials in Experiment 1; 25.79% in Experiment 2), and no-choice/not-preferred trials (25.67% of included trials in Experiment 1; 25.79% in Experiment 2)...

"We modeled the data using the brm function of the brms package (Version 2.13.0) in the R programming environment (BÃ¼rkner, 2017, 2018), fitting a cumulative-logit model for the ordinal response data from Experiment 1...

"The dependent variable in Experiment 1 was curiosity rating, an ordinal variable containing the values 1, 2, 3, and 4...

"The independent variables of interest were selection condition (choice preferred, no choice/preferred, and no choice/not preferred)...

"Selection condition was a factor with three levels, for which we created two sum-to-zero contrasts. We contrasted choice-preferred with no-choice/preferred trials to assess the effect of choice, and no-choice/preferred with no-choice/not-preferred trials to assess the effect of preference. With categorical predictors, brms provides estimates only for each contrast and not for the factor as a whole. Given that we were interested in disentangling the effects of choice and preference, this was suitable. Hence, in the results of the analysis with brms, we report statistics only for each contrast and not for the factor of selection condition as a whole...

We fitted the models using four chains with 10,000 iterations (5,000 for warm-up) and inspected the model summaries for convergence (paying attention to ð‘…Ë† and effective-sample-size values) as well as the plots for chain iterations. We used the default priors of the brms package (Cauchy priors and Lewandowski-Kurowicka-Joe priors for correlation parameters; BÃ¼rkner, 2017). We considered coefficients to be statistically significant if the associated 95% posterior credible intervals did not overlap with zero" (Verdugo et al., 2022).

Thus, the key analysis of interest is the contrast in self-reported curiosity between Choice/Preferred and No-Choice/Preferred conditions. The contrast takes the form of a coefficient reported by brms function, and the key statistical test is the 95% confidence interval for this coefficient. If it does not overlap with zero, then the contrast can be considered statistically significant.

### Differences from Original Study

There were several notable deviations from the original study, all of which are listed below.

-   Only Experiment 1 from the original study will be replicated.
- Only the key claim of this experiment, (the contrast in self-reported curiosity between Choice/Preferred and No-Choice/Preferred conditions) will be tested. Curiosity as a function of expected value and outcome uncertainty will not be discussed.
-   The experiment was remote, rather than in the lab, took roughly 8.5 minutes to complete on average, and was implemented using Qualtrics. The original experiment was done in the lab and took roughly an hour and a half.
- The original sample consisted of people from the Netherlands. The replication was done with US participants.
-   Participants do not give a verbal summary of instructions. Instead, they acknowledge having read written instructions and participate in an interactive tutorial with two practice trials (one choice and one no-choice) rather than 20 practice trials as in the original study.
- Bonus payment are directly proportional to number of points, ie. for every 10 points, a participant earns 1 cent bonus. The average bonus was about 50 cents per participant.
-   A smaller selection of lotteries was included due to time constraint. There are 6 unique lotteries, so 12 total games (one choice and one no-choice game for each lottery).
-   There is no explicit time pressure to the survey in order to give participants time to answer questions accurately, taking into account that the initial training is far less intensive than in the original study, and the setting is remote rather than in-person. This should prevent haphazard responses that may occur with timed questions and missed responses due to auto-advancement. With that being said, the data exclusion criteria do exclude participants who completed the survey unreasonably fast or slow.

### Methods Addendum (Post Data Collection)

#### Actual Sample

The actual sample (after exclusions) was N = 31. Participants were age 20-40, English speaking, and of US nationality.

I closely followed the data preparation and exclusion criteria outlined in the original paper, with some exceptions. The original exlcusions are as follows:

"Thirty-seven healthy individuals participated in Experiment 1, and another 40 healthy individuals participated in Experiment 2. Following our preregistration (https://osf.io/gseqn/), we excluded participants who missed more than 10% of total trials (Criterion 1). Additionally, we excluded participants who scored poorly according to at least one of two other performance criteria. Criterion 2 was when a participant gave the same curiosity rating in more than two thirds of their responses in Experiment 1 or made the same willingness-to-wait decision in more than 90% of their responses in Experiment 2. Criterion 3 was when a participant ranked lower than 3 standard deviations below the mean in choice-preference coherence (i.e., frequency of trials on which a participant chose the same vase that they had previously indicated that they preferred). We excluded three participants from Experiment 1 (one on the basis of Criterion 2 and two on the basis of Criterion 3) and six participants from Experiment 2 (five on the basis of Criterion 2 and one on the basis of Criterion 3). No participants were excluded on the basis of Criterion 1. The final sample in Experiment 1 (N = 34) included 26 women (age: M = 23.5 years, SD = 4.5); the final sample in Experiment 2 (N = 34) included 24 women (age: M = 23.5 years, SD = 3.1)" (Verdugo et al., 2022).

They continue:

"We filtered out trials in which participants missed or did not respond within the allocated times for any of the three required responses (0.37% of total trials in Experiment 1; 0.37% in Experiment 2). Furthermore, we filtered out choice trials in which participants had chosen a lottery other than their preferred one (2.73% of total trials in Experiment 1; 2.93% in Experiment 2). Because we expected participants to exhibit their preferences through their choices, we assumed that inconsistencies were due to changes of mind or response errors.
We sorted the remaining trials into three selection conditions: choice-preferred trials (48.61% of included trials in Experiment 1; 48.42% in Experiment 2), no-choice/preferred trials (25.72% of included trials in Experiment 1; 25.79% in Experiment 2), and no-choice/not-preferred trials (25.67% of included trials in Experiment 1; 25.79% in Experiment 2)" (Verdugo et al., 2022).

#### Differences from pre-data collection methods plan

For Criterion 1, instead of excluding participants who missed more than 10% of 116 trials (as in the original experiment), I simply excluded those who did not finish all of 6 trials (12 total games) in this experiment. Since this experiment is far shorter in duration, contains fewer trials than the original, and is not subject to timed auto-advancement, it can be subjected to a stricter exclusion criterion. I followed Criterion 2 of excluding participants who gave the same curiosity rating in more than two thirds of their responses. Finally, for Criterion 3, I simply excluded participants whose choice of vase did not match their preference in more than half of trials (more than 3 of 6 trials) for reasons similar to the changes made to Criterion 1.

Since the survey questions are not themselves timed, I instead discount responses from participants who took far too long to complete the entire survey (>18 minutes) and those who were unreasonably fast (<4.5 minutes) as the average time to complete the survey was about 8.5 minutes. Within participant responses, I do not include trials in which participants chose a different vase than the one they preferred. Finally, I sort the remaining data into the same three selection conditions.

## Results

### Data Preparation

All of my code for reading-in, processing, and analyzing data is documented below.

```{r}
# load data from the Qualtrics .csv file
library(tidyverse)
library(dplyr)
data <- read.csv('/Users/austinweideman/Desktop/choice_curiosity_data.csv')
```

```{r}
# here we organize all relevant data (eg. selection conditions, trial number, and curiosity ratings) for all trials and all participants, into one larger table

# initialize the table
all_participant_trials <- data.frame(matrix(ncol = 5, nrow = 0))

# maintain another table which keeps track of excluded participants and reasons for exclusion
excluded_participants <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(excluded_participants) <- cbind('Excluded Participant', 'Reason')

# iterate over desired rows in raw Qualtrics csv file (one row corresponds to one participant)
i = 1
num_participants = 0
for (row in 18:nrow(data)) {
  participant_row <- data[c(row),]
  participant <- row-2
  num_participants = num_participants + 1

  duration <- strtoi(participant_row['Duration..in.seconds.'])
  finished <- participant_row['Finished']
  
  # move to next participant row if duration of survey is unreasonably fast or slow
  # or if current participant did not complete survey
  if (duration < 270) {
    excluded_part <- c(participant, 'Too Fast')
    excluded_participants[nrow(excluded_participants)+1,] <- excluded_part
    num_participants = num_participants - 1
    next
  }
  if (duration > 1080) {
    excluded_part <- c(participant, 'Too Slow')
    excluded_participants[nrow(excluded_participants)+1,] <- excluded_part
    num_participants = num_participants - 1
    next
  }
  if (finished == 'False') {
    excluded_part <- c(participant, 'Unfinished')
    excluded_participants[nrow(excluded_participants)+1,] <- excluded_part
    num_participants = num_participants - 1
    next
  }
  # create a smaller table containing all of the relevant trial data for the current participant
  t1 = participant_row %>% dplyr::select(T1.C.Preference, T1.Choice.Vase, T1.C.Curiosity_1, T1.NC.Curiosity_1, T1Pref)
  t2 = participant_row %>% dplyr::select(T2.C.Preference, T2.Choice.Vase, T2.C.Curiosity_1, T2.NC.Curiosity_1, T2Pref)
  t3 = participant_row %>% dplyr::select(T3.C.Preference, T3.Choice.Vase, T3.C.Curiosity_1, T3.NC.Curiosity_1, T3Pref)
  t4 = participant_row %>% dplyr::select(T4.C.Preference, T4.Choice.Vase, T4.C.Curiosity_1, T4.NC.Curiosity_1, T4Pref)
  t5 = participant_row %>% dplyr::select(T5.C.Preference, T5.Choice.Vase, T5.C.Curiosity_1, T5.NC.Curiosity_1, T5Pref)
  t6 = participant_row %>% dplyr::select(T6.C.Preference, T6.Choice.Vase, T6.C.Curiosity_1, T6.NC.Curiosity_1, T6Pref)
  part_trials_data <- t1
  part_trials_data[nrow(part_trials_data) + 1,] = t2
  part_trials_data[nrow(part_trials_data) + 1,] = t3
  part_trials_data[nrow(part_trials_data) + 1,] = t4
  part_trials_data[nrow(part_trials_data) + 1,] = t5
  part_trials_data[nrow(part_trials_data) + 1,] = t6
  temp1 <- cbind(Trial = c('T1', 'T2', 'T3', 'T4', 'T5', 'T6'), part_trials_data)
  part_trials_data <- cbind(Participant = participant, temp1)
  
  # append the participant's trial table to the larger table containing all participant data
  all_participant_trials = rbind(all_participant_trials, part_trials_data)
}
colnames(all_participant_trials) <- c('Participant', 'Trial', 'C_Preference', 'Choice', 'C_Curiosity', 'NC_Curiosity', 'NC_Preferred')
rownames(all_participant_trials) <- NULL

# essential data from all 6 trials for all participants
all_participant_trials
```

```{r}
# clean data by excluding participants who gave the same curiosity rating for more than two thirds of their responses

filtered_participant_trials <- all_participant_trials
num_valid_participants <- num_participants
participants <- all_participant_trials$Participant
participants <- participants[!duplicated(participants)]

for (participant in participants) {
  num1 <- 0
  num2 <- 0
  num3 <- 0
  num4 <- 0
  part_data <- all_participant_trials %>% filter(Participant == participant)
  for (row in 1: nrow(part_data)) {
    if (part_data[row, 'C_Curiosity'] == '1') {
      num1 <- num1 + 1
    }
    if (part_data[row, 'NC_Curiosity'] == '1') {
      num1 <- num1 + 1
    }
    if (part_data[row, 'C_Curiosity'] == '2') {
      num2 <- num2 + 1
    }
    if (part_data[row, 'NC_Curiosity'] == '2') {
      num2 <- num2 + 1
    }
    if (part_data[row, 'C_Curiosity'] == '3') {
      num3 <- num3 + 1
    }
    if (part_data[row, 'NC_Curiosity'] == '3') {
      num3 <- num3 + 1
    }
    if (part_data[row, 'C_Curiosity'] == '4') {
      num4 <- num4 + 1
    }
    if (part_data[row, 'NC_Curiosity'] == '4') {
      num4 <- num4 + 1
    }
  }
  counts <- c(num1, num2, num3, num4)
  if (max(counts) > 8) {
    # participant gave the same rating in more than 8/12 (2/3) of games
    filtered_participant_trials <- filtered_participant_trials %>% filter(Participant != participant)
    excluded_part <- c(participant, 'Too many similar responses')
    excluded_participants[nrow(excluded_participants)+1,] <- excluded_part
    num_valid_participants <- num_valid_participants - 1
  }
}

```

```{r}
# exclude participants whose preference does not match their choice in more than half of trials

participants <- filtered_participant_trials$Participant
participants <- participants[!duplicated(participants)]

for (participant in participants) {
  part_data <- filtered_participant_trials %>% filter(Participant == participant)
  choice_and_pref_not_matching <- part_data %>% filter(C_Preference != Choice)
  if (nrow(choice_and_pref_not_matching) > 3) {
    filtered_participant_trials <- filtered_participant_trials %>% filter(Participant != participant)
    excluded_part <- c(participant, 'Too many choice/preference mismatches')
    excluded_participants[nrow(excluded_participants)+1,] <- excluded_part
    num_valid_participants <- num_valid_participants - 1
  }
}
# total number of all valid participants after all exclusions
num_valid_participants

# all excluded participants and reasons for exclusion
excluded_participants

```

```{r}
# further clean data by excluding all remaining trials in which preference does not match choice
filtered_participant_trials <- filtered_participant_trials %>% filter(C_Preference == Choice)

# we can now make this data more compact by removing vase information since it is no longer necessary
filtered_participant_trials <- subset(filtered_participant_trials, select = -c(C_Preference, Choice))

# final filtered table of all relevant trial data for all valid participants
filtered_participant_trials
```
```{r}
# convert this table to long form for easier analysis
# we will have one column for conditions, and one for curiosity

filtered_participant_trials_long <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(filtered_participant_trials_long) <- c('Participant', 'Trial', 'Conditions', 'Curiosity')

for (row in 1:nrow(filtered_participant_trials)) {
  part_row <- filtered_participant_trials[row,]
  participant <- part_row[1, 'Participant']
  trial <- part_row[1, 'Trial']
  c_cur <- strtoi(part_row[1, 'C_Curiosity'])
  nc_cur <- strtoi(part_row[1, 'NC_Curiosity'])
  new_choice_row <- c(participant, trial, 'Choice/Preferred', c_cur)
  filtered_participant_trials_long[nrow(filtered_participant_trials_long)+1,] = new_choice_row
  nc_condition <- part_row[1, 'NC_Preferred']
  if (nc_condition == 'True') {
    new_nc_pref_row <- c(participant, trial, 'No_Choice/Preferred', nc_cur)
    filtered_participant_trials_long[nrow(filtered_participant_trials_long)+1,] = new_nc_pref_row
  }
  if (nc_condition == 'False') {
    new_nc_not_pref_row <- c(participant, trial, 'No_Choice/Not_Preferred', nc_cur)
    filtered_participant_trials_long[nrow(filtered_participant_trials_long)+1,] = new_nc_not_pref_row
  }
}
# convert values to numeric for later analysis
filtered_participant_trials_long$Curiosity <- as.numeric(filtered_participant_trials_long$Curiosity)
filtered_participant_trials_long


```
```{r}
# convert to wide format for contrasts with brms
# set contrast codes for three variables: is.choice_pref, is.nochoice.pref, and is.nochoice_notpref

filtered_participant_trials_wide <- data.frame(matrix(ncol = 6, nrow = 0))
colnames(filtered_participant_trials_wide) <- c('Participant', 'Trial', 'is.choice_pref', 'is.nochoice_pref', 'is.nochoice_notpref', 'Curiosity')

for (row in 1:nrow(filtered_participant_trials)) {
  part_row <- filtered_participant_trials[row,]
  participant <- part_row[1, 'Participant']
  trial <- part_row[1, 'Trial']
  c_cur <- strtoi(part_row[1, 'C_Curiosity'])
  nc_cur <- strtoi(part_row[1, 'NC_Curiosity'])
  
  # set contrast code for a choice/preferred game
  new_choice_row <- c(participant, trial, 0.5, 0, NA, c_cur)
  # add the choice/preferred game to the table
  filtered_participant_trials_wide[nrow(filtered_participant_trials_wide)+1,] = new_choice_row
  
  nc_condition <- part_row[1, 'NC_Preferred']
  if (nc_condition == 'True') {
    # set contrast code for a no choice/preferred game
    new_nc_pref_row <- c(participant, trial, -0.5, 0.5, NA, nc_cur)
    filtered_participant_trials_wide[nrow(filtered_participant_trials_wide)+1,] = new_nc_pref_row
  }
  if (nc_condition == 'False') {
    # set contrast code for a no choice/not preferred game
    new_nc_not_pref_row <- c(participant, trial, 0, -0.5, NA, nc_cur)
    filtered_participant_trials_wide[nrow(filtered_participant_trials_wide)+1,] = new_nc_not_pref_row
  }
}
# convert to numeric
filtered_participant_trials_wide$is.choice_pref <- as.numeric(filtered_participant_trials_wide$is.choice_pref)
filtered_participant_trials_wide$is.nochoice_pref <- as.numeric(filtered_participant_trials_wide$is.nochoice_pref)
filtered_participant_trials_wide$is.nochoice_notpref <- as.numeric(filtered_participant_trials_wide$is.nochoice_notpref)
filtered_participant_trials_wide$Curiosity <- as.numeric(filtered_participant_trials_wide$Curiosity)
filtered_participant_trials_wide
```

```{r}
# select all choice/preferred games
choice_preferred_games_long <- filtered_participant_trials_long %>% filter(Conditions == 'Choice/Preferred')
choice_preferred_games_long
# average choice/preferred curiosity rating and standard deviation
avg_all_c_curiosity <- mean(choice_preferred_games_long$Curiosity)
sd_all_c_curiosity <- sd(choice_preferred_games_long$Curiosity)
avg_all_c_curiosity
sd_all_c_curiosity

# select all no-choice/preferred games for analysis
nc_preferred_trials <- filtered_participant_trials %>% filter(NC_Preferred == 'True')
nc_preferred_games_long <- filtered_participant_trials_long %>% filter(Conditions == 'No_Choice/Preferred')
nc_preferred_games_long
# average no choice/preferred curiosity rating and standard deviation
avg_all_nc_pref_curiosity <- mean(nc_preferred_games_long$Curiosity)
sd_all_nc_pref_curiosity <- sd(nc_preferred_games_long$Curiosity)
avg_all_nc_pref_curiosity
sd_all_nc_pref_curiosity

# select all no-choice/not preferred games for analysis
nc_not_pref_trials <- filtered_participant_trials %>% filter(NC_Preferred == 'False')
nc_not_pref_games_long <- filtered_participant_trials_long %>% filter(Conditions == 'No_Choice/Not_Preferred')
nc_not_pref_games_long
# average no choice/not preferred curiosity rating and standard deviation
avg_all_nc_not_pref_curiosity <- mean(nc_not_pref_games_long$Curiosity)
sd_all_nc_not_pref_curiosity <- sd(nc_not_pref_games_long$Curiosity)
avg_all_nc_not_pref_curiosity
sd_all_nc_not_pref_curiosity
```

```{r}
# obtain plot of average curiosities across all participants
df <- data.frame(Curiosity= c(round(avg_all_c_curiosity, 2), round(avg_all_nc_pref_curiosity, 2), round(avg_all_nc_not_pref_curiosity, 2)), Conditions = c('Choice/Preferred', 'No Choice/Preferred', 'No Choice/Not Preferred'))
library(ggplot2)
plot <- ggplot(data=df, aes(x= reorder(Conditions, -Curiosity), y=Curiosity)) +
  geom_bar(width = 0.7, stat="identity", fill=c('#D2042D', '#0047AB', '#808080'))+
  geom_text(aes(label=Curiosity), hjust = 2, vjust=1.6, color="white", size=3.5)+
  theme_minimal()
plot <- plot + ggtitle("Average Curiosity Ratings Across All Participants") +
  xlab("") + ylab("Curiosity Rating") + theme(plot.title = element_text(hjust = 0.5)) + scale_y_continuous(breaks = seq(0, 4.0, 0.5)) + theme(plot.title = element_text(color = 'black', size = 14, face = "bold"))
```

```{r}
# isolate score, reward, and prolific id for each participant
# commented out to avoid showing identifiable information online
# participant_rewards <- data.frame(matrix(ncol = 3, nrow = 0))
# 
# for (row in 18:nrow(data)) {
#   participant_row <- data[c(row),]
#   score_info = participant_row %>% dplyr::select(PROLIFIC_PID, Score, Reward)
#   scores <- cbind(Participant = row-2, score_info)
#   participant_rewards = rbind(participant_rewards, scores)
# }
# rownames(participant_rewards) <- NULL
# participant_rewards
```

### Key Statistic

```{r}
# to obtain the key statistic, run brm with curiosity modeled on selection conditions
library(brms)
set.seed(1)

brm.fit <- brm(Curiosity ~ is.choice_pref + is.nochoice_pref, data = filtered_participant_trials_wide, family = cumulative("logit"), chains = 4, iter = 10000, warmup = 5000)

# the key statistic we are interested in is the 95% CI associated with the coefficient representing the contrast in curiosity ratings between choice/preferred and no choice/preferred conditions, the one labeled 'is.choice_pref'

# note, brm gives a slightly different 95% CI every time, so the results noted in the analysis may be different than what is shown here in the code upon knitting

summary(brm.fit)
```

### Confirmatory Analysis

The key statistic is the associated 95% posterior credible interval for the contrast in curiosity ratings (using brms with family cumulative logit) between choice/preferred and no choice/preferred conditions.

Original 95% CI = [0.46, 0.71]

Replication 95% CI = [-0.05, 0.99]

The graphs below display the results of the original study and those of the replication.

![Fig 1: Original Graph](/Users/austinweideman/Desktop/originalgraph.png)

"The mean curiosity rating in each of the three selection conditions in Experiment 1 is shown as a function of (a) expected value and (b) outcome uncertainty. Curiosity ratings were given on a scale from 1 (not curious) to 4 (very curious)" (Verdugo et al., 2022).


```{r}
# replication graph
plot
```
Fig 2: Replication Graph

Mean curiosity ratings in all three selection conditions.

## Discussion

### Summary of Replication Attempt

The results of the replication were consistent with those of the original study in that average curiosity was highest in choice/preferred conditions, followed by no choice/preferred, and then no choice/not preferred. As you can see in the graphs above, the results are consistent (note that the original graph also displays curiosity as a function of expected value and outcome uncertainty, but the replication does not). However, the key finding of the original study that there was a statistically significant contrast in curiosity ratings between choice/preferred conditions and no choice/preferred conditions only partially replicated. The original study found that "choice enhanced curiosity; participants were more curious about lotteries that they had chosen themselves... This was the case when curiosity was measured explicitly (brms contrast: 95% CI = [0.46, 0.71]" (Verdugo et al, 2022). However, in this replication, the associated 95% CI ([-0.05, 0.99]) for the same contrast overlaps with zero, so the results of this replication cannot be considered statistically significant. However, the overlap is not large. It could therefore be the case that with more data, obtained either by recruiting more participants or by adding more trials to the survey, that this experiment would yield significant results. The large drop in participants from 83 to 31 due to exclusion criteria was unforeseen and is one reason for the deficit in data that may have led to these results. One potential cause for the large drop in participants were the differences in training and in-person environment. Potential improvements to the survey could include attention checks and adding more total trials.
